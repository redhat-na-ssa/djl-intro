:scrollbar:
:data-uri:
:toc2:
:linkattrs:


= DJL on Quarkus Java

:numbered:

This project uses Quarkus, the _Supersonic Subatomic Java Framework_ as well as DJL, the _Deep Java Library_.

If you want to learn more about Quarkus, please visit its link:https://quarkus.io[website].

If you want to learn more about DJL, please visit its link:https://djl.ai[website]

== Overview
The project demonstrates serving of a deep learning _image classification_ application.
In particular, the application can predict the probability that a link:https://djl-ai.s3.amazonaws.com/resources/images/kitten_small.jpg[kitten] depicted in an image is some sub-species of the cat family.

To do so, the same link:https://github.com/redhat-na-ssa/djl-intro/blob/main/djl-iclassification/src/main/java/org/acme/apps/ImageClassificationResource.java[simple app] can be powered (mostly via configuration) by various Deep Learning frameworks (ie: _Tensorflow_, _PyTorch_, _MXNet_) as link:https://djl.ai/docs/engine.html#supported-engines[supported by DJL].



This project introduces the following _deep learning_ capabilities of DJL :

. *Image Classification*
. *Live Object Detection*


Once the apps are running, you interact with them via their RESTful endpoints.

image::docs/images/djl-iclassification-swagger.png[]

== Local
This application can run in your local environment.

=== Pre-reqs

. *GPU/CPU*
+
This app will auto-detect the presence of a GPU and utilize it if found.
Otherwise, the app will default to a CPU.
+
The app exposes various diagnostic related REST endpoints that provide insight as to the GPU/CPU it has found.

. *Network*
+
You'll need a reliable broadband network due to downloading of a large quantity of library dependencies.

. *JDK11* (or more recent)
+
ie: `dnf install java-latest-openjdk-devel`

. *maven*
+
ie: `dnf install maven`

. *cURL* (or similar http test utility)
+
ie: `dnf install curl`

. *DJL_CACHE_DIR*
+
DJL engines link:https://djl.ai/docs/development/cache_management.html[download models] and any needed C++ shared object files to a directory specified by the environment variable: `DJL_CACHE_DIR` .  
You are encouraged to set this environment variable in your shell.  
Otherwise, DJL will write these files to: `$HOME/.djl.ai`

=== Quarkus `dev mode`

The application can be run locally in quarkus `dev mode` (which enables live coding).

NOTE:  a GPU is not required to run the application.  If deep learning engine detects the presence of a GPU, then it will use it.  If not, the deep learning engine will default to CPUs.

. Seed the `maven wrapper`:
+
-----
$ mvn wrapper:wrapper
-----

. Run the application in quarkus `dev mode`` using any of the following deep learning engines:


.. *TensorFlow*
+
```
./mvnw quarkus:dev -Ptensorflow
```

.. *PyTorch*
+
-----
./mvnw quarkus:dev -Ppytorch
-----

.. *MXNet*
+
-----
./mvnw quarkus:dev -Pmxnet
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`


== Linux Containers

=== Pre-reqs:

. *podman*
+
ie:  `dnf install podman`

. *quay.io*
+
Linux container images already exist in `quay.io`.
If you want to push to quay.io, then authenticate as follows:
+
-----
$ podman login quay.io
-----


. To support link:https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/configurations.md#djl-settings[off-line mode] of the DJL engines, a pre-seeded DJL cache will be mounted to the linux container.  
emporary directories and/or json files might be generated in this DJL cache.  
This DJL cache directory should be made writable by the container process for the following reasons:

.. Extraction of native C++ libraries included in DJL `fatjar` to $DJL_CACHE_DIR
.. Downloading of any models from DJL's ModelZoo that may be used by the application.

. Make $DJL_CACHE_DIR writable for container process:
+
-----
$ export DJL_CACHE_DIR_OCI=/u02/djl.ai.oci \
    && mkdir -p $DJL_CACHE_DIR_OCI

$ sudo semanage fcontext -a \
        -t container_file_t "$DJL_CACHE_DIR_OCI(/.*)?"

$ sudo restorecon -R $DJL_CACHE_DIR_OCI

$ podman unshare chown -R 185:185 $DJL_CACHE_DIR_OCI
-----

=== Image Classification

==== Create Linux Container

. Change directory into:  `djl-iclassification`

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Build container and generate openshift/helm configs:
+
-----
$ cp config/kube/openshift-$djl_engine.yml src/main/kubernetes/openshift.yml \
  && ./mvnw clean package \
            -P$djl_engine \
            -Dquarkus.application.name=djl-iclassification-$djl_engine \
            -DskipTests \
            -Dquarkus.container-image.build=true \
            -Dquarkus.container-image.push=true
-----


==== Execution

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Set environment variable indicating whether to run the DJL engine in offline mode:
+
-----
$ djl_offline=false
-----

. Run linux container using designated deep learning engine:
+
-----
$ podman run \
    --rm \
    --name djl-iclass-$djl_engine \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline" \
    -e DJL_CACHE_DIR=/mnt/djl.ai \
    -v $DJL_CACHE_DIR_OCI:/mnt/djl.ai:z \
    quay.io/redhat_naps_da/djl-iclassification-$djl_engine:0.0.3
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`

=== Object Detection

==== Create Linux Container

. Change directory into:  `djl-objectdetect`

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Build container and generate openshift/helm configs:
+
-----
$ ./mvnw clean package \
            -P$djl_engine \
            -Dquarkus.application.name=djl-objectdetect-$djl_engine \
            -DskipTests \
            -Dquarkus.container-image.build=true \
            -Dquarkus.container-image.push=true
-----

==== Execution

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Set environment variable indicating whether to run the DJL engine in offline mode:
+
-----
$ djl_offline=false
-----

. Specify the video card to capture from:
+
-----
$ djl_video=0
-----

. The `djl-objectdetection` container needs access to the host's video card.
+
Podman allows for that however you need to ensure that your host operating system user is a member of the `video` group:
+
-----
$ sudo usermod -a -G video <your OS user name>
-----

. The `djl-objectdetection` container needs the ability to write images to the filesystem of the host.
Subsequently, in this step you enable the local filesystem to be writable by the container:
+
-----
$ I_DIR=/tmp/org.acme.objectdetection/ \
    && mkdir -p $I_DIR

$ sudo semanage fcontext -a \
        -t container_file_t "$I_DIR(/.*)?"

$ sudo restorecon -R $I_DIR

$ podman unshare chown -R 185:185 $I_DIR
-----


. Run linux container using designated deep learning engine:
+
NOTE: podman utility provides access to the host machine's video card as per link:https://www.redhat.com/sysadmin/files-devices-podman[this document]
+
-----
$ podman run \
    --rm \
    --name djl-objectdetect-$djl_engine \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline -Dorg.acme.objectdetection.video.capture.device.id=$djl_video" \
    -e DJL_CACHE_DIR=/mnt/djl.ai \
    -v $DJL_CACHE_DIR_OCI:/mnt/djl.ai:z \
    --device /dev/video$djl_video \
    --group-add keep-groups \
    -v /tmp/org.acme.objectdetection:/tmp/org.acme.objectdetection:z \
    -v ./config/application.properties:/deployments/config/application.properties:z \
    quay.io/redhat_naps_da/djl-objectdetect-$djl_engine:0.0.1
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`

=== Fingerprint Serving

==== Create Linux Container

. Change directory into:  `djl-fprint`

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=tensorflow
-----
+
NOTE:  The model (and subsequently the engine) is Tensorflow specific

. Set an environment variable that specifies the processor type:
+
-----
$ djl_processor=gpu
-----
+
NOTE: Options are:  `cpu` and `gpu`

. Build container and generate openshift/helm configs:
+
-----
$ cp config/kube/openshift-$djl_processor.yml src/main/kubernetes/openshift.yml \
  && ./mvnw clean package \
        -P$djl_engine-$djl_processor \
        -Dquarkus.application.name=djl-fprint-$djl_engine-$djl_processor \
        -DskipTests \
        -Dquarkus.container-image.build=true \
        -Dquarkus.container-image.push=true
-----

==== Execution

. Set path to model path on local filesystem:
+
-----
$ djl_model_dir=$HOME/Downloads/fingerprint/fingerprint/1
-----
+
NOTE:  Model can be downloaded link:https://github.com/redhat-na-ssa/demo-datasci-fingerprint-data[here].

. Set environment variable indicating whether to run the DJL engine in offline mode:
+
-----
$ djl_offline=true
-----

. Run linux container using designated deep learning engine:
+
-----
$ podman run \
    --rm \
    --name djl-fprint-$djl_engine-$djl_processor \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline" \
    -v $djl_model_dir:/mnt/model:z \
    quay.io/redhat_naps_da/djl-fprint-$djl_engine-$djl_processor:0.0.1
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`

== OpenShift

=== Pre-reqs

. *OpenShift Container Platform*
.. Tested on OCP 4.12 beta  (but earlier versions should also work fine as well)
.. CPU:
+
Allow 1 cpu core for each deep learning engine deployed.
+
Currently not tested using a GPU.
.. RAM:
+
Allow 1Gb RAM for each deep learning engine deployed.

.. Storage:  no PVs needed

. *helm*
+
ie: `dnf install helm`

. *cURL* (or similar http test utility)
+
ie: `dnf install curl`

=== Procedure

==== Deploy

. Create a ConfigMap from the project's _application.properties_:
+
-----
$ oc create cm djl-iclassification --from-file=config/application.properties
-----

. Deploy app powered by PyTorch:
+
-----
$ helm install djl-iclassification-pytorch https://github.com/redhat-na-ssa/djl-intro/raw/main/helm/djl-iclassification-pytorch-0.0.1.tar.gz
-----

. Deploy app powered by TensorFlow:
+
-----
$ helm install djl-iclassification-tensorflow https://github.com/redhat-na-ssa/djl-intro/raw/main/helm/djl-iclassification-tensorflow-0.0.1.tar.gz
-----

. Deploy app powered by Apache MXNet:
+
-----
$ helm install djl-iclassification-mxnet https://github.com/redhat-na-ssa/djl-intro/raw/main/helm/djl-iclassification-mxnet-0.0.1.tar.gz
-----

. Determine node that pod landed on:
+
-----
$ oc get pod \
    -l deploymentconfig=djl-iclassification-pytorch \
    -o json \
    -n user1-services \
    | jq -r .items[0].spec.nodeName
-----
+
NOTE: The result should return the id of your GPU enabled node.

==== Test

. Check your routes:
+
-----
$ $ oc get route

NAME                     HOST/PORT                                                            PATH   SERVICES                 PORT   TERMINATION   WILDCARD
djl-iclassification-mxnet        djl-iclassification-mxnet-user1-services.apps.den-east12.ratwater.xyz        /      djl-iclassification-mxnet        http                 None
djl-iclassification-pytorch      djl-iclassification-pytorch-user1-services.apps.den-east12.ratwater.xyz      /      djl-iclassification-pytorch      http                 None
djl-iclassification-tensorflow   djl-iclassification-tensorflow-user1-services.apps.den-east12.ratwater.xyz   /      djl-iclassification-tensorflow   http                 None
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to the output of the following:
+
-----
/q/swagger-ui`
-----



== Future:  Linux Native 
You can create a native executable using: 

```baseh
# use PyTorch engine
./mvnw clean package -Pnative -Ppytorch

# use TensorFlow engine
./mvnw clean package -Pnative -Ptensorflow
```

Or, if you don't have GraalVM installed, you can run the native executable build in a container using: 

```
./mvnw clean package -Pnative -Ppytorch -Dquarkus.native.container-build=true
```

You can then execute your native executable with:
 
```
target/imageclassification-1.0.0-SNAPSHOT-runner

# Turn on tensorflow javacpp debug log 
target/imageclassification-1.0.0-SNAPSHOT-runner -Dorg.bytedeco.javacpp.logger.debug=true
```

If you want to learn more about building native executables, please consult https://quarkus.io/guides/building-native-image.

== Alternatives

. link:https://docs.djl.ai/docs/serving/index.html[DJL Serving]
+
DJL Serving is a high performance universal stand-alone model serving solution powered by DJL. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint.

. link:https://camel.apache.org/components/3.20.x/djl-component.html[Camel-DJL]

== Quarkus project seed

-----
mvn io.quarkus:quarkus-maven-plugin:2.14.0.Final:create \
        -DprojectGroupId=org.acme \
        -DprojectArtifactId=djl-objectdetect \
        -Dextensions="quarkus-resteasy-reactive, quarkus-resteasy-reactive-jackson, quarkus-smallrye-health, quarkus-container-image-docker, quarkus-openshift, quarkus-kubernetes-config, quarkus-smallrye-openapi" \
        -DplatformVersion=2.14.0.Final
-----

== `fatjar`

DJL and Quarkus allow the ability to create a _fat_ jar (aka: _uber-jar).
Not sure this adds much value in off-line mode.
*-native dependencies specified in pom.xml provide the C++ *.so files and are extracted to $DJL_CACHE_DIR

=== Reference
. link:https://github.com/deepjavalibrary/djl-demo/tree/master/development/fatjar[djl-demo fatjar] example

=== Procedure

. Configure project `pom.xml` such that the dependency that provides the engine's native C++ shared object files matches the target environment where execution will occur.  ie: for mxnet on a CPU environment running linux, the dependency needs to be:
+
-----
        <dependency>
            <groupId>ai.djl.mxnet</groupId>
            <artifactId>mxnet-native-mkl</artifactId>
            <classifier>linux-x86_64</classifier>
        </dependency>
-----

. Create a quarkus based uber jar:
+
-----
$ ./mvnw clean package \
    -P$djl_engine \
    -Dquarkus.application.name=djl-objectdetect-$djl_engine \
    -DskipTests \
    -Dquarkus.package.type=uber-jar \
    -Dquarkus.container-image.build=true \
    -Dquarkus.container-image.builder=jib
-----

. Execute uber jar and test in `offline` mode:

.. Turn off internet to host machine

.. Execute:

.. JVM:
+
-----
$ java -Doffline=true -jar target/djl-objectdetect-0.0.1-runner.jar
-----
+
You should see log entries similar to the following pertaining to the engine's native libs:
+
-----
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libc10.so to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libgomp-52f2fd74.so.1 to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libtorch.so to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libtorch_cpu.so to cache ...
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libgomp-52f2fd74.so.1
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libc10.so
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libtorch_cpu.so
04:55:47 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libtorch.so
04:55:47 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting jnilib/linux-x86_64/cpu/libdjl_torch.so to cache ...
04:55:47 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/0.20.0-libdjl_torch.so
-----
+
Notice that native shared libraries packaged in the uber jar are extracted to $DJL_CACHE_DIR (or $HOME/.djl.ai if $DJL_CACHE_DIR is not set)

.. linux container:
+
NOTE: podman utility provides access to the host machine's video card as per link:https://www.redhat.com/sysadmin/files-devices-podman[this document]
+
-----
$ podman run \
    --rm \
    --name djl-objectdetect-$djl_engine \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline" \
    --device /dev/video0 \
    --group-add keep-groups \
    -v /tmp/org.acme.objectdetection:/tmp/org.acme.objectdetection:z \
    -v ./config/application.properties:/deployments/config/application.properties:z \
    quay.io/redhat_naps_da/djl-objectdetect-$djl_engine:0.0.1
-----

== Questions

. Compare link:https://djl.ai/docs/development/inference_performance_optimization.html[DJL threading/performance] with scalability link:https://developer.nvidia.com/nvidia-triton-inference-server#scalable-ai[claims] of NVIDIA Triton
