:scrollbar:
:data-uri:
:toc2:
:linkattrs:


= DJL on Quarkus Java

:numbered:

This project uses Quarkus, the _Supersonic Subatomic Java Framework_ as well as Amazon's DJL, the _Deep Java Library_.

This project supports the topics discussed in the presentation: link:https://docs.google.com/presentation/d/1itd3Nj2vqeR6TK0Rkr5dsagVT7TpEuNu3h1zIw3lRv8/edit#slide=id.ga66cd00dcb_0_0[Deep Java Library on a Red Hat Stack].



== Overview
The project demonstrates serving of the following _deep learning_ applications created using Quarkus and Amazon's _Deep Java Library_.


. *link:docs/README-djl-iclassification.adoc[Cat Species Image Classification]*
. _Intelligence at the Edge_ via *link:docs/README-djl-objectdetect.adoc[Live Object Detection]*
. *link:docs/README-djl-fprint.adoc[Fingerprint Detection]*

== Reference
. If you want to learn more about Quarkus, please visit its link:https://quarkus.io[website].
. If you want to learn more about _Deep Java Library_, please visit its link:https://djl.ai[website]

== Appendix

=== Future:  Linux Native 
You can create a native executable using: 

```baseh
# use PyTorch engine
./mvnw clean package -Pnative -Ppytorch

# use TensorFlow engine
./mvnw clean package -Pnative -Ptensorflow
```

Or, if you don't have GraalVM installed, you can run the native executable build in a container using: 

```
./mvnw clean package -Pnative -Ppytorch -Dquarkus.native.container-build=true
```

You can then execute your native executable with:
 
```
target/imageclassification-1.0.0-SNAPSHOT-runner

# Turn on tensorflow javacpp debug log 
target/imageclassification-1.0.0-SNAPSHOT-runner -Dorg.bytedeco.javacpp.logger.debug=true
```

If you want to learn more about building native executables, please consult https://quarkus.io/guides/building-native-image.

=== Alternatives

. link:https://docs.djl.ai/docs/serving/index.html[DJL Serving]
+
DJL Serving is a high performance universal stand-alone model serving solution powered by DJL. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint.

. link:https://camel.apache.org/components/3.20.x/djl-component.html[Camel-DJL]

=== Quarkus project seed

-----
mvn io.quarkus:quarkus-maven-plugin:2.14.0.Final:create \
        -DprojectGroupId=org.acme \
        -DprojectArtifactId=djl-objectdetect \
        -Dextensions="quarkus-resteasy-reactive, quarkus-resteasy-reactive-jackson, quarkus-smallrye-health, quarkus-container-image-docker, quarkus-openshift, quarkus-kubernetes-config, quarkus-smallrye-openapi" \
        -DplatformVersion=2.14.0.Final
-----

=== `fatjar`

DJL and Quarkus allow the ability to create a _fat_ jar (aka: _uber-jar).
Not sure this adds much value in off-line mode.
*-native dependencies specified in pom.xml provide the C++ *.so files and are extracted to $DJL_CACHE_DIR.

link:https://github.com/deepjavalibrary/djl-demo/tree/master/development/fatjar[djl-demo fatjar] example






==== Procedure

. Configure project `pom.xml` such that the dependency that provides the engine's native C++ shared object files matches the target environment where execution will occur.  ie: for mxnet on a CPU environment running linux, the dependency needs to be:
+
-----
        <dependency>
            <groupId>ai.djl.mxnet</groupId>
            <artifactId>mxnet-native-mkl</artifactId>
            <classifier>linux-x86_64</classifier>
        </dependency>
-----

. Create a quarkus based uber jar:
+
-----
$ ./mvnw clean package \
    -P$djl_engine \
    -Dquarkus.application.name=djl-objectdetect-$djl_engine \
    -DskipTests \
    -Dquarkus.package.type=uber-jar \
    -Dquarkus.container-image.build=true \
    -Dquarkus.container-image.builder=jib
-----

. Execute uber jar and test in `offline` mode:

.. Turn off internet to host machine

.. Execute:

.. JVM:
+
-----
$ java -Doffline=true -jar target/djl-objectdetect-0.0.1-runner.jar
-----
+
You should see log entries similar to the following pertaining to the engine's native libs:
+
-----
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libc10.so to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libgomp-52f2fd74.so.1 to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libtorch.so to cache ...
04:55:45 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting pytorch/cpu/linux-x86_64/libtorch_cpu.so to cache ...
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libgomp-52f2fd74.so.1
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libc10.so
04:55:46 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libtorch_cpu.so
04:55:47 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/libtorch.so
04:55:47 INFO  [ai.dj.py.jn.LibUtils] (main) Extracting jnilib/linux-x86_64/cpu/libdjl_torch.so to cache ...
04:55:47 DEBUG [ai.dj.py.jn.LibUtils] (main) Loading native library: /home/jbride/.djl.ai/pytorch/1.13.0-20221116-cpu-linux-x86_64/0.20.0-libdjl_torch.so
-----
+
Notice that native shared libraries packaged in the uber jar are extracted to $DJL_CACHE_DIR (or $HOME/.djl.ai if $DJL_CACHE_DIR is not set)

.. linux container:
+
NOTE: podman utility provides access to the host machine's video card as per link:https://www.redhat.com/sysadmin/files-devices-podman[this document]
+
-----
$ podman run \
    --rm \
    --name djl-objectdetect-$djl_engine \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline" \
    --device /dev/video0 \
    --group-add keep-groups \
    -v /tmp/org.acme.objectdetection:/tmp/org.acme.objectdetection:z \
    -v ./config/application.properties:/deployments/config/application.properties:z \
    quay.io/redhat_naps_da/djl-objectdetect-$djl_engine:0.0.1
-----

-----
$ ./mvnw clean package \
            -Dquarkus.application.name=djl-objectdetect-web \
            -DskipTests \
            -Dquarkus.container-image.build=true \
            -Dquarkus.container-image.push=true
-----

-----
$ podman run \
    --rm \
    --name djl-objectdetect-web \
    -p 9080:9080 \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Dmp.messaging.incoming.liveObjectDetection.host=$HOSTNAME -Dquarkus.http.port=9080" \
    quay.io/redhat_naps_da/djl-objectdetect-web:0.0.1
-----

=== Questions

. Compare link:https://djl.ai/docs/development/inference_performance_optimization.html[DJL threading/performance] with scalability link:https://developer.nvidia.com/nvidia-triton-inference-server#scalable-ai[claims] of NVIDIA Triton
